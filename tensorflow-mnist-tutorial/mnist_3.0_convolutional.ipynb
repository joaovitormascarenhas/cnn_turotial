{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Convolucionais (CNN)\n",
    "*Algumas partes foram traduzidas pelo Google Transalte (usando Redes Neurais Recorrentes RNN, famoso modelo Seq2Seq) \n",
    "## O que inspirou as redes convolucionais?\n",
    "\n",
    "CNNs são modelos de inspiração biológica inspirados na pesquisa de D. H. Hubel e T. N. Wiesel. Eles propuseram uma explicação sobre a maneira pela qual os mamíferos percebem visualmente o mundo ao seu redor usando uma arquitetura em camadas de neurônios no cérebro, o que, por sua vez, inspirou engenheiros a tentar desenvolver mecanismos similares de reconhecimento de padrões em visão computacional.\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-235acb60a481423eaf70c39b17bc914b.webp \"Logo Title Text 1\")\n",
    "\n",
    "Em sua hipótese, dentro do córtex visual, as respostas funcionais complexas geradas por \"células complexas\" são construídas a partir de respostas mais simples de \"células simples\".\n",
    "\n",
    "Para casos, células simples responderiam a bordas orientadas, etc., enquanto células complexas também responderiam a bordas orientadas, mas com um grau de invariância espacial.\n",
    "\n",
    "Existem campos receptivos para células, onde uma célula responde a uma soma de entradas de outras células locais.\n",
    "\n",
    "A arquitetura das CNNs foi inspirada pelas idéias mencionadas acima\n",
    "- conexões locais\n",
    "- camadas\n",
    "- invariância espacial (a mudança do sinal de entrada resulta em um sinal de saída igualmente deslocado.) A maioria de nós é capaz de reconhecer faces específicas sob uma variedade de condições porque aprendemos abstração. Essas abstrações são invariantes ao tamanho, contraste, rotação, orientação\n",
    " \n",
    "No entanto, continua a ser visto se esses mecanismos computacionais de redes neurais convolutivas são semelhantes aos mecanismos de computação que ocorrem no sistema visual primata\n",
    "\n",
    "\n",
    "\n",
    "## NN vs CNN\n",
    "De acordo com LeCun et al. (1998), as redes convolucionas diferem das redes neurais mais simples pois contém:\n",
    "1. Campos receptivos locais: os neurônios de uma camada não estão ligados a todos os neurônios da camada anterior ou posterior, apenas nos neurônios dentro da sua \"janela de visão\" (Receptive Field) \n",
    "2. Pesos compartilhados (ou replicação de peso): neurônios em uma mesma camada utilizam o filtro(máscara/kernel) com os mesmo pesos.\n",
    "3. Subamostragem espacial ou temporal: As camadas vão diminuíndo o tamanho da entrada, mas acrescentam maior significado a ela.\n",
    "\n",
    "Além disto, CNNs possuem camadas de Convolução e Pooling.\n",
    "\n",
    "# Como elas funcionam?\n",
    "- Ver vídeo [https://www.youtube.com/watch?v=gB_-LabED68]\n",
    "\n",
    "\n",
    "## Arquitetura\n",
    "Podemos dividir uma CNN em duas partes:\n",
    "- 1. Aprendizagem automática de features\n",
    "- 2. Classificação\n",
    "![alt text](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1497876372993.jpg \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## Etapa 1 - Preparando o conjunto de dados de imagens\n",
    "\n",
    "![](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png)\n",
    "\n",
    "- Cada imagem é uma matriz de valores de pixel.\n",
    "- Mais comumente, temos pixels de 8 bits ou 1 tamanho de byte. Assim, a possível gama de valores que um único pixel pode representar é [0, 255].\n",
    "- No entanto, com imagens coloridas, particularmente imagens RGB (Red, Green, Blue), a presença de canais de cores separados (3 no caso de imagens RGB) introduz um campo adicional de \"profundidade\" para os dados, tornando a entrada tridimensional.\n",
    "- Portanto, para uma determinada imagem RGB de tamanho, digamos pixels 255 × 255 (Largura x Altura), teremos 3 matrizes associadas a cada imagem, uma para cada um dos canais de cores.\n",
    "- Assim, a imagem na sua totalidade, constitui uma estrutura tridimensional chamada **Volume de Entrada** (255x255x3).\n",
    "\n",
    "### Dataset MNIST\n",
    "Neste turotial iremos usar o dataset MNIST de dígitos de 0-9 escritos a mão. Este conjunto contém imagens escala de cinza de 28x28 pixels com 60K imagens para treino e 10K para teste. Assim, como estamos trabalhando com uma imagem em escala de cinza e apenas um canal de intensidade, nosso **Volume de Entrada** será de **(28x28x1)**.\n",
    "![](https://chrisjmccormick.files.wordpress.com/2015/01/layer_1.png)\n",
    "\n",
    "### Importando as bibliotecas necessárias e baixando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.1.0\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowvisu\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Download images and labels into mnist.test (10K images+labels) and mnist.train (60K images+labels)\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Convolution \n",
    "- Um kernel (também chamado de filtro, feature map ou matriz de pesos) é uma matriz de valores reais de tamanho menor em comparação com as dimensões de entrada da imagem. Tal Kernel ou Filtro pode ser definido para ressaltar bordas, causar desfoque Gaussiano, ou qualquer outra operação.\n",
    "\n",
    "- A comcolução do kernel é um processo onde passa-se essa matriz de numeros sobre a imagem calculando uma multiplicação ponto a ponto e subtituindo o ponto central, na nova imagem, pela soma das multiplicações. Desta forma, a imagem original é transformada em uma nova imagem, de acordo com os valores do kernel. \n",
    "\n",
    "![alt text](https://s3-eu-west-1.amazonaws.com/com.cambridgespark.content/tutorials/convolutional-neural-networks-with-keras/figures/convolve.png)\n",
    "\n",
    "![](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/Convolution_schematic.gif \"Logotipo Texto Texto 1\")\n",
    "\n",
    "## Etapa 2 - Camada de Convolução\n",
    "\n",
    "- Uma convolução é um procedimento em que duas fontes de informação serão entrelaçadas. É combinar duas coisas para criar uma nova (pode-se pensar em convolução como sinônimo para combinação)\n",
    "\n",
    "- É calculada a convolução dos **kernels** com o **volume de entrada** para obter os chamados **\"mapas de ativação\"** (também chamados de mapas de features).\n",
    "\n",
    "- Os mapas de ativação indicam regiões \"ativadas\", ou seja, regiões onde os recursos específicos do kernel foram detectados na entrada.\n",
    "\n",
    "![](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_2.png \"Texto do título do logotipo 1\")\n",
    "![](http://i.imgur.com/g4hRI6Z.png \"Texto do título do logotipo 1\")\n",
    "![Alt text](http://i.imgur.com/tpQvMps.jpg \"Logotipo Título Texto 1\")\n",
    "![Alt text](http://i.imgur.com/oyXkhHi.jpg \"Logotipo Título Texto 1\")\n",
    "\n",
    "- Os valores reais da matriz do kernel mudam a cada iteração de aprendizagem sobre o conjunto de treinamento, indicando que a rede está aprendendo a identificar quais regiões são importantes para extrair recursos dos dados.\n",
    "\n",
    "- A \"janela\" do kernel é então deslizada pela imagem, de acordo com um \"passo\" **(stride)**, e o processo é repetido até que toda a imagem de entrada tenha sido processada. *O processo é realizado para todos os canais de cores.\n",
    "\n",
    "- Em vez de conectar cada neurônio a todos os pixels possíveis, especificamos uma região bidimensional denominada \"campo receptivo [14]\" (digamos de tamanho 5 × 5 unidades) que se estende para toda a profundidade da entrada (5x5x3 para um canal de 3 cores Entrada), dentro dos quais os pixels abrangidos estão totalmente conectados à camada de entrada da rede neural. É sobre essas pequenas regiões que as seções transversais da camada de rede (cada uma constituídas por vários neurônios (chamadas \"colunas de profundidade\") operam e produzem o mapa de ativação. (Reduz a complexidade computacional)\n",
    "![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/53c160301db12a6e.png)\n",
    "\n",
    "\n",
    "![Alt text] (http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png)\n",
    "\n",
    "Grande recurso sobre a descrição da convolução (discreto versus contínuo) e a transformada de Fourier\n",
    "Http://timdettmers.com/2015/03/26/convolution-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3 - Camada de Pooling (Agrupamento)\n",
    "![](http://cs231n.github.io/assets/cnn/maxpool.jpeg)\n",
    "- A camada de pooling reduz as dimensões espaciais (largura x altura) do volume de entrada para a próxima camada convolucional. Não afeta a dimensão de profundidade do Volume (o número de canais, por exemplo).\n",
    "- A transformação é realizada tomando o valor máximo dos valores observáveis na janela (chamada de 'Max Pooling'), ou tomando a média dos valores. O **Max Pooling** é mais usado em relação a outros devido às suas características de melhor desempenho.\n",
    "- Esta etapa também é chamada de **Downsampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4 - Normalização (ReLU no nosso caso)\n",
    "\n",
    "A função de normalização **ReLU** (Rectified Linear Unit) é a mais usada pois faz com que o modelo seja capaz de aprender funções não lineares mais eficientemente. Basicamente, ela transforma números negativos em 0, uma pilha de imagens torna-se uma pilha de imagens sem valores negativos.\n",
    "![](http://xrds.acm.org/blog/wp-content/uploads/2016/06/CodeCogsEqn-3.png)\n",
    "![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/60cac06459b3cc08.png)\n",
    "\n",
    "\n",
    "\n",
    "## Etapa 5 - Dropout (abandono)\n",
    "\n",
    "- Dropout força uma rede neural artificial para aprender múltiplas representações independentes dos mesmos dados alternadamente, desativando aleatoriamente os neurônios na fase de aprendizagem.\n",
    "- O abandono é uma característica vital em quase todas as implementações de rede neural de última geração.\n",
    "- Para executar o abandono de uma camada, você ajusta aleatoriamente alguns dos valores da camada para 0 durante a propagação direta.\n",
    "See [this](http://iamtrask.github.io/2015/07/28/dropout/)\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/CewjH.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## Etapa 6 - Conversão de Probabilidades\n",
    "\n",
    "No final da nossa rede, aplicaremos uma função softmax para converter as saídas em valores de probabilidade para cada classe.\n",
    "![](./images/softmax.png)\n",
    "![] (https://1.bp.blogspot.com/-FHDU505euic/Vs1iJjXHG0I/AAAAAAABVKg/x4g0FHuz7_A/s1600/softmax.JPG \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## Etapa 7 - Escolha a classe mais provável (valor máximo de probabilidade)\n",
    "\n",
    "Argmax (softmax_outputs)\n",
    "\n",
    "Essas 7 etapas são uma passagem pela rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por quê adicionas mais camadas?\n",
    "Somando pequenas informações a cada camada você consegue ir compondo conceitor e acrescentando contexto\n",
    "![alt text](./images/deep_abstraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Então, como aprendemos as matrizes de pesos?\n",
    "\n",
    "- Podemos aprender as matrizes de peso (kernels) por meio de backpropagation\n",
    "\n",
    "![alt text](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/images/cover.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-52-638.jpg?cb=1455889178 \"Logo Title Text 1\")\n",
    "\n",
    "Os outros hiperparâmetros são definidos pelos seres humanos e são um campo de pesquisa ativo (achando os melhores)\n",
    "\n",
    "Ex.: número de neurônios, número de camadas, número de features, tamanho dos features, tamanho do kernel de pooling, tamanho do kernel de convolução, passo em pixels do kernel **stride**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura escolhida no tutorial\n",
    " \n",
    "- Costuma-se usar camadas de **pooling** para diminuir a dimensão da entrada. No entanto, neste tutorial os cientistas do Google indicam empiricamente que utilizar apenas camadas de convolução alterando o passo (**stride**), os resultados são melhores. Nada pré-definido ou geral, apenas decisão de projeto (hiperparâmetro).\n",
    "\n",
    "![alt text](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/3701df765a81a094.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network structure for this sample:\n",
    "#\n",
    "# · · · · · · · · · ·      (input data, 1-deep)                 X [batch, 28, 28, 1]\n",
    "# @ @ @ @ @ @ @ @ @ @   -- conv. layer 5x5x1=>4 stride 1        W1 [5, 5, 1, 4]        B1 [4]\n",
    "# ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                           Y1 [batch, 28, 28, 4]\n",
    "#   @ @ @ @ @ @ @ @     -- conv. layer 5x5x4=>8 stride 2        W2 [5, 5, 4, 8]        B2 [8]\n",
    "#   ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                             Y2 [batch, 14, 14, 8]\n",
    "#     @ @ @ @ @ @       -- conv. layer 4x4x8=>12 stride 2       W3 [4, 4, 8, 12]       B3 [12]\n",
    "#     ∶∶∶∶∶∶∶∶∶∶∶                                               Y3 [batch, 7, 7, 12] => reshaped to YY [batch, 7*7*12]\n",
    "#      \\x/x\\x\\x/        -- fully connected layer (relu)         W4 [7*7*12, 200]       B4 [200]\n",
    "#       · · · ·                                                 Y4 [batch, 200]\n",
    "#       \\x/x\\x/         -- fully connected layer (softmax)      W5 [200, 10]           B5 [10]\n",
    "#        · · ·                                                  Y [batch, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialização\n",
    "\n",
    "Variable: Graus de liberdade do sistema, o que queremos que o TensorFlow compute (aprenda) para nós durante o treino. Neste caso são os pesos **W** e bias (tendências) **B**. Os pesos são inicializados com valores randômicos.\n",
    "![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/40fd4b6ad8dfb6d2.png)\n",
    "\n",
    "Placeholder: Training data que será dada a rede durante o treino. Neste caso, as imagens de entreda X e os resultados das convoluções Y. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "#                           <[BatchSIZE, Dimensoes das imagens de entrada]>\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "# three convolutional layers with their channel counts, and a\n",
    "# fully connected layer (tha last layer has 10 softmax neurons)\n",
    "K = 4  # first convolutional layer output depth\n",
    "L = 8  # second convolutional layer output depth\n",
    "M = 12  # third convolutional layer\n",
    "N = 200  # fully connected layer\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![] (https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/604a9797da2a48d7.png)\n",
    "\n",
    "![] (https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/206327168bc85294.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model\n",
    "stride = 1  # output is 28x28\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2  # output is 14x14\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2  # output is 7x7\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "#A FUNCAO RESHAPE ACHATA A IMAGEM PARA \n",
    "# reshape the output from the third convolution for the fully connected layer5 k\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a Cross Entropy\n",
    "- Cross entropy é a distancia entre a predição da rede e o que a entrada realmente é. O label da entrada é representado com um vetor de zeros com um 1 apenas na classe da imagem, essa representação é chamada de **\"one-hot\" encoding**\n",
    "![](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/1d8fc59e6a674f1c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# matplotlib visualisation\n",
    "allweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\n",
    "allbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\n",
    "I = tensorflowvisu.tf_format_mnist_images(X, Y, Y_)\n",
    "It = tensorflowvisu.tf_format_mnist_images(X, Y, Y_, 1000, lines=25)\n",
    "datavis = tensorflowvisu.MnistDataVis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de otimização\n",
    "- Nesse passo, definimos uma função de otimizaçao que o TensorFlow usará para diminuir o erro (**cross entropy**) durante o treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.06 loss: 240.954 (lr:0.003)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0955 test loss: 236.164\n",
      "20: accuracy:0.7 loss: 85.4854 (lr:0.0029711445178725875)\n",
      "40: accuracy:0.81 loss: 65.6362 (lr:0.0029425761525895904)\n",
      "60: accuracy:0.93 loss: 24.0883 (lr:0.0029142920472906737)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-674d5783c44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# to disable the visualisation use the following line instead of the datavis.animate line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max test accuracy: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatavis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-674d5783c44d>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(i, update_test_data, update_train_data)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# O LEARNING RATE LIMITA O TAMANHO DO PASSO QUE VOCÊ DARÁ NO GRADIENTE,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# ISSO, PARA GARANTIR QUE VOCE NAO PULE DE VALES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#datavis.animate(training_step, 10001, train_data_update_freq=10, test_data_update_freq=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joaovitorms/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joaovitorms/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joaovitorms/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/joaovitorms/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/joaovitorms/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training step, the learning rate is a placeholder\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# You can call this function in a loop to train the model, 100 images at a time\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "\n",
    "    # learning rate decay\n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, im, w, b = sess.run([accuracy, cross_entropy, I, allweights, allbiases], {X: batch_X, Y_: batch_Y})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        datavis.append_training_curves_data(i, a, c)\n",
    "        datavis.update_image1(im)\n",
    "        datavis.append_data_histograms(i, w, b)\n",
    "\n",
    "    # compute test values for visualisation\n",
    "    if update_test_data:\n",
    "        a, c, im = sess.run([accuracy, cross_entropy, It], {X: mnist.test.images, Y_: mnist.test.labels})\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        datavis.append_test_curves_data(i, a, c)\n",
    "        datavis.update_image2(im)\n",
    "\n",
    "    # the backpropagation training step\n",
    "    #AQUI É EXECUTADA UMA COMPUTAÇAO DO TENSOR FLOW, COM UM BATCH DE IMAGENS\n",
    "    #PASSA-SE PRA ELE OS PLACEHOLDERS\n",
    "    # O LEARNING RATE LIMITA O TAMANHO DO PASSO QUE VOCÊ DARÁ NO GRADIENTE, \n",
    "    # ISSO, PARA GARANTIR QUE VOCE NAO PULE DE VALES\n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate})\n",
    "\n",
    "#datavis.animate(training_step, 10001, train_data_update_freq=10, test_data_update_freq=100)\n",
    "\n",
    "# to save the animation as a movie, add save_movie=True as an argument to datavis.animate\n",
    "# to disable the visualisation use the following line instead of the datavis.animate line\n",
    "\n",
    "for i in range(10000+1): training_step(i, i % 100 == 0, i % 20 == 0)\n",
    "\n",
    "print(\"max test accuracy: \" + str(datavis.get_max_test_accuracy()))\n",
    "\n",
    "# layers 4 8 12 200, patches 5x5str1 5x5str2 4x4str2 best 0.989 after 10000 iterations\n",
    "# layers 4 8 12 200, patches 5x5str1 4x4str2 4x4str2 best 0.9892 after 10000 iterations\n",
    "# layers 6 12 24 200, patches 5x5str1 4x4str2 4x4str2 best 0.9908 after 10000 iterations but going downhill from 5000 on\n",
    "# layers 6 12 24 200, patches 5x5str1 4x4str2 4x4str2 dropout=0.75 best 0.9922 after 10000 iterations (but above 0.99 after 1400 iterations only)\n",
    "# layers 4 8 12 200, patches 5x5str1 4x4str2 4x4str2 dropout=0.75, best 0.9914 at 13700 iterations\n",
    "# layers 9 16 25 200, patches 5x5str1 4x4str2 4x4str2 dropout=0.75, best 0.9918 at 10500 (but 0.99 at 1500 iterations already, 0.9915 at 5800)\n",
    "# layers 9 16 25 300, patches 5x5str1 4x4str2 4x4str2 dropout=0.75, best 0.9916 at 5500 iterations (but 0.9903 at 1200 iterations already)\n",
    "# attempts with 2 fully-connected layers: no better 300 and 100 neurons, dropout 0.75 and 0.5, 6x6 5x5 4x4 patches no better\n",
    "#*layers 6 12 24 200, patches 6x6str1 5x5str2 4x4str2 dropout=0.75 best 0.9928 after 12800 iterations (but consistently above 0.99 after 1300 iterations only, 0.9916 at 2300 iterations, 0.9921 at 5600, 0.9925 at 20000)\n",
    "# layers 6 12 24 200, patches 6x6str1 5x5str2 4x4str2 no dropout best 0.9906 after 3100 iterations (avove 0.99 from iteration 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
